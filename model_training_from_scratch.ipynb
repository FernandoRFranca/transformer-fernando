{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33533516",
   "metadata": {},
   "source": [
    "## Transformer From Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fbb5b06",
   "metadata": {},
   "source": [
    "### Transformer Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8fcedb",
   "metadata": {},
   "source": [
    "#### Next Token Prediction Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "370ef06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.init as init\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ca23236",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_length):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros((max_seq_length, d_model))\n",
    "        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b63e7e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0, \"Vetor de embedding precisa ser divisivel pelo número de cabeças da camada de atenção!\"\n",
    "        self.head_dim = d_model // num_heads\n",
    "        self.d_model, self.num_heads = d_model, num_heads\n",
    "        self.q = nn.Linear(d_model, d_model)\n",
    "        self.k = nn.Linear(d_model, d_model)\n",
    "        self.v = nn.Linear(d_model, d_model)\n",
    "        self.output_linear = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def split_heads(self, x, encoder_output=None):\n",
    "        # Entra Q, K, V com dimensão (batch_size, sequence_length, d_model)\n",
    "        # Reshape para (batch_size, num_heads, sequence_length, d_model)\n",
    "        # Reordering para (batch_size, num_heads, sequence_length, d_model)\n",
    "        if encoder_output is None:\n",
    "            x = torch.reshape(x, shape=(x.shape[0], self.num_heads, x.shape[1], self.head_dim)) #.contiguous()\n",
    "        else:\n",
    "            raise NotImplementedError(\"Modelo ainda não compatível com Encoder.\")\n",
    "        return x\n",
    "\n",
    "    def compute_attention_scores(self, q_linear_out, k_linear_out, v_linear_out, mask=None):\n",
    "        qk_dot_product = torch.matmul(q_linear_out, k_linear_out.transpose(2, 3)) / self.head_dim ** 0.5\n",
    "\n",
    "        if mask is not None:\n",
    "            qk_dot_product = qk_dot_product.masked_fill(mask == 0, float('-inf'))\n",
    "\n",
    "        attn_scores = nn.functional.softmax(qk_dot_product, dim=-1)\n",
    "        attn_weighted_v = torch.matmul(attn_scores, v_linear_out)\n",
    "\n",
    "        return attn_weighted_v\n",
    "\n",
    "\n",
    "    def combine_heads(self, x):\n",
    "        return torch.reshape(x, shape=(x.shape[0], x.shape[2], int(x.shape[1] * x.shape[3]))).contiguous()\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        q_linear_out = self.split_heads(self.q(x))\n",
    "        k_linear_out = self.split_heads(self.k(x))\n",
    "        v_linear_out = self.split_heads(self.v(x))\n",
    "        \n",
    "        attn_weighted_v = self.compute_attention_scores(q_linear_out, k_linear_out, v_linear_out, mask=mask)\n",
    "        attn_weighted_v = self.combine_heads(attn_weighted_v)\n",
    "        return self.output_linear(attn_weighted_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "88b513ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 8, 512, 512])\n"
     ]
    }
   ],
   "source": [
    "matrix_1 = torch.rand(1, 8, 512, 10)\n",
    "matrix_2 = torch.rand(1, 8, 512, 10)\n",
    "print(torch.matmul(matrix_1, matrix_2.transpose(-1, -2)).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a54c4b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardSubLayer(nn.Module):\n",
    "    def __init__(self, d_model, hidden_size):\n",
    "        super().__init__()\n",
    "        self.ff_1 = nn.Linear(d_model, hidden_size)\n",
    "        self.ff_2 = nn.Linear(hidden_size, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.ff_2(self.relu(self.ff_1(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "591863be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, d_model, hidden_size, num_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.feed_forward = FeedForwardSubLayer(d_model, hidden_size)\n",
    "        self.mha = MultiHeadAttention(d_model, num_heads) # nn.MultiheadAttention()\n",
    "        self.norm_1 = nn.LayerNorm(d_model)\n",
    "        self.norm_2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, tgt_mask):\n",
    "        x = self.norm_1(x + self.dropout(self.mha(x, tgt_mask)))\n",
    "        x = self.norm_2(x + self.dropout(self.feed_forward(x)))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "350f1b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, max_sequence_length, n_layers, hidden_size, num_heads, dropout=0.1):\n",
    "        super(TransformerDecoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=d_model)\n",
    "        self.pe = PositionalEncoding(d_model, max_sequence_length)\n",
    "        self.layers = nn.ModuleList(\n",
    "            [DecoderBlock(d_model, hidden_size, num_heads, dropout) for _ in range(n_layers)]\n",
    "        )\n",
    "        self.output_layer = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x, tgt_mask):\n",
    "        x = self.embedding(x)\n",
    "        x = self.pe(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, tgt_mask)\n",
    "        out = self.output_layer(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "34c574e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 259 tokens possíveis:\n",
    "# ASCII + <UNK> + <SOS> + <EOS> + <PAD>\n",
    "\n",
    "class TokenizerChar:\n",
    "    def __init__(self):\n",
    "        self.chr_to_idx = {chr(v): v for v in range(1, 257)}\n",
    "        self.chr_to_idx['<SOS>'] = 257\n",
    "        self.chr_to_idx['<EOS>'] = 258\n",
    "        self.chr_to_idx['<PAD>'] = 0\n",
    "        self.chr_to_idx['<UNK>'] = 259\n",
    "\n",
    "        self.idx_to_chr = {v: k for k, v in self.chr_to_idx.items()}\n",
    "\n",
    "        self.vocab_size = len(self.chr_to_idx.keys())\n",
    "\n",
    "    def encode(self, char):\n",
    "        if char in self.chr_to_idx.keys():\n",
    "            return self.chr_to_idx[char]\n",
    "        else:\n",
    "            return 259\n",
    "    \n",
    "    def decode(self, token_idx):\n",
    "        return self.idx_to_chr[token_idx]\n",
    "    \n",
    "    def sos_token(self):\n",
    "        return '<SOS>'\n",
    "    \n",
    "    def sos_token_idx(self):\n",
    "        return self.chr_to_idx['<SOS>']\n",
    "\n",
    "    def eos_token(self):\n",
    "        return '<EOS>'\n",
    "    \n",
    "    def eos_token_idx(self):\n",
    "        return self.chr_to_idx['<EOS>']\n",
    "    \n",
    "    def pad_token(self):\n",
    "        return '<PAD>'\n",
    "    \n",
    "    def pad_token_idx(self):\n",
    "        return self.chr_to_idx['<PAD>']\n",
    "    \n",
    "    def get_vocab_size(self):\n",
    "        return self.vocab_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a67c0c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class DatasetDialogs(Dataset):\n",
    "    def __init__(self, dataset_path, sentence_length):\n",
    "        self.dataset_path = dataset_path\n",
    "        self.sentence_length = sentence_length\n",
    "        self.tokenizer = TokenizerChar()\n",
    "\n",
    "    def __len__(self):\n",
    "        with open(self.dataset_path, 'r') as dataset:\n",
    "            num_of_sentences = len(dataset.read().split('\\n'))\n",
    "        return num_of_sentences\n",
    "    \n",
    "    def get_shape(self):\n",
    "        with open(self.dataset_path, 'r') as dataset:\n",
    "            num_of_sentences = len(dataset.read().split('\\n'))\n",
    "        return (num_of_sentences, self.sentence_length)\n",
    "\n",
    "    def __getitem__(self, line_idx):\n",
    "        with open(self.dataset_path, 'r') as dataset:\n",
    "            selected_sentence = dataset.read().split('\\n')[line_idx]\n",
    "            if len(selected_sentence) < self.sentence_length:\n",
    "                input_tokens = [self.tokenizer.sos_token_idx()] + [self.tokenizer.encode(char) for char in selected_sentence]\n",
    "                input_tokens.append(self.tokenizer.eos_token_idx())\n",
    "                pad_length = self.sentence_length - len(input_tokens) + 1\n",
    "                pad_tokens = [self.tokenizer.pad_token_idx()] * pad_length\n",
    "                input_tokens += pad_tokens\n",
    "            elif len(selected_sentence) == self.sentence_length:\n",
    "                input_tokens = [self.tokenizer.sos_token_idx()] + [self.tokenizer.encode(char) for char in selected_sentence]\n",
    "                input_tokens[-1] = self.tokenizer.eos_token_idx()\n",
    "            elif len(selected_sentence) > self.sentence_length:\n",
    "                selected_sentence = selected_sentence[:self.sentence_length]\n",
    "                input_tokens = [self.tokenizer.sos_token_idx()] + [self.tokenizer.encode(char) for char in selected_sentence]\n",
    "                input_tokens[-1] = self.tokenizer.eos_token_idx()\n",
    "            # print(f'{len(input_tokens)} - {self.sentence_length}') # debug only\n",
    "            assert len(input_tokens) == self.sentence_length + 1, f\"Lista de índices de tokens não possui mesmo tamanho que 'sentence_length'! len(input_tokens): {len(input_tokens)} - self.sentence_length: {self.sentence_length}\"\n",
    "            try:\n",
    "                x = torch.tensor(input_tokens[:-1])\n",
    "                y = torch.tensor(input_tokens[1:])\n",
    "            except RuntimeError as e:\n",
    "                print(e)\n",
    "                print(f\"Input tokens: {input_tokens}\")\n",
    "                raise e\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a21c8e64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Starting model training...\n",
      "Epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 107/107 [00:18<00:00,  5.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average epoch training loss: 3.1735775448451533\n",
      "Last batch training loss: 2.7660892009735107\n",
      "Epoch validation loss: 2.6767372846603394\n",
      "Epoch: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 107/107 [00:18<00:00,  5.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average epoch training loss: 2.580989764115521\n",
      "Last batch training loss: 2.481311798095703\n",
      "Epoch validation loss: 2.4199381828308106\n",
      "Epoch: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 107/107 [00:18<00:00,  5.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average epoch training loss: 2.4017743930638393\n",
      "Last batch training loss: 2.3187801837921143\n",
      "Epoch validation loss: 2.303889751434326\n",
      "Epoch: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 107/107 [00:18<00:00,  5.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average epoch training loss: 2.28008285861149\n",
      "Last batch training loss: 2.1804916858673096\n",
      "Epoch validation loss: 2.1839465618133547\n",
      "Epoch: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 107/107 [00:18<00:00,  5.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average epoch training loss: 2.157245627073484\n",
      "Last batch training loss: 2.099167823791504\n",
      "Epoch validation loss: 2.0813934564590455\n",
      "Epoch: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 107/107 [00:19<00:00,  5.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average epoch training loss: 2.0475467042388202\n",
      "Last batch training loss: 2.074234962463379\n",
      "Epoch validation loss: 1.9872362971305848\n",
      "Epoch: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 107/107 [00:20<00:00,  5.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average epoch training loss: 1.950556634742523\n",
      "Last batch training loss: 1.8038610219955444\n",
      "Epoch validation loss: 1.9098064064979554\n",
      "Epoch: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 107/107 [00:20<00:00,  5.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average epoch training loss: 1.870883761165298\n",
      "Last batch training loss: 1.8429877758026123\n",
      "Epoch validation loss: 1.8441076040267945\n",
      "Epoch: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 107/107 [00:20<00:00,  5.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average epoch training loss: 1.8056340384706158\n",
      "Last batch training loss: 1.823001742362976\n",
      "Epoch validation loss: 1.8065894365310669\n",
      "Epoch: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 107/107 [00:21<00:00,  5.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average epoch training loss: 1.7506003446668108\n",
      "Last batch training loss: 1.669348955154419\n",
      "Epoch validation loss: 1.770089852809906\n",
      "Epoch: 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 107/107 [00:21<00:00,  5.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average epoch training loss: 1.6962462819625284\n",
      "Last batch training loss: 1.6796238422393799\n",
      "Epoch validation loss: 1.747429883480072\n",
      "Epoch: 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 107/107 [00:21<00:00,  5.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average epoch training loss: 1.652255013724354\n",
      "Last batch training loss: 1.5694397687911987\n",
      "Epoch validation loss: 1.7166443705558776\n",
      "Epoch: 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 107/107 [00:21<00:00,  4.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average epoch training loss: 1.6114624415602639\n",
      "Last batch training loss: 1.6831997632980347\n",
      "Epoch validation loss: 1.6984066724777223\n",
      "Epoch: 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 107/107 [00:21<00:00,  4.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average epoch training loss: 1.5713848889431106\n",
      "Last batch training loss: 1.4785460233688354\n",
      "Epoch validation loss: 1.6779712915420533\n",
      "Epoch: 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 107/107 [00:21<00:00,  4.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average epoch training loss: 1.5383966147342576\n",
      "Last batch training loss: 1.4607819318771362\n",
      "Epoch validation loss: 1.6440895080566407\n",
      "Epoch: 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 107/107 [00:22<00:00,  4.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average epoch training loss: 1.5069312046621448\n",
      "Last batch training loss: 1.5084620714187622\n",
      "Epoch validation loss: 1.6411741733551026\n",
      "Epoch: 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 107/107 [00:21<00:00,  5.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average epoch training loss: 1.478037105542477\n",
      "Last batch training loss: 1.4786267280578613\n",
      "Epoch validation loss: 1.6250544905662536\n",
      "Epoch: 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 107/107 [00:21<00:00,  4.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average epoch training loss: 1.4514438437524242\n",
      "Last batch training loss: 1.382041573524475\n",
      "Epoch validation loss: 1.6198889136314392\n",
      "Epoch: 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 107/107 [00:21<00:00,  5.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average epoch training loss: 1.4256321559442537\n",
      "Last batch training loss: 1.4371919631958008\n",
      "Epoch validation loss: 1.6077964663505555\n",
      "Epoch: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 107/107 [00:21<00:00,  4.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average epoch training loss: 1.4022864381843638\n",
      "Last batch training loss: 1.4393110275268555\n",
      "Epoch validation loss: 1.6071891784667969\n",
      "Epoch: 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 107/107 [00:21<00:00,  4.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average epoch training loss: 1.3799509467365585\n",
      "Last batch training loss: 1.4499001502990723\n",
      "Epoch validation loss: 1.595920741558075\n",
      "Epoch: 22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 107/107 [00:21<00:00,  4.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average epoch training loss: 1.3591369212230788\n",
      "Last batch training loss: 1.3573778867721558\n",
      "Epoch validation loss: 1.5823466777801514\n",
      "Epoch: 23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 107/107 [00:21<00:00,  4.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average epoch training loss: 1.3381482942082057\n",
      "Last batch training loss: 1.3940792083740234\n",
      "Epoch validation loss: 1.585340976715088\n",
      "Epoch: 24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 107/107 [00:21<00:00,  4.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average epoch training loss: 1.320044987669615\n",
      "Last batch training loss: 1.3111461400985718\n",
      "Epoch validation loss: 1.5944262623786927\n",
      "Epoch: 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 107/107 [00:21<00:00,  5.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average epoch training loss: 1.3019802470073523\n",
      "Last batch training loss: 1.2995924949645996\n",
      "Epoch validation loss: 1.5765890717506408\n",
      "Epoch: 26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 107/107 [00:21<00:00,  5.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average epoch training loss: 1.2816898622245432\n",
      "Last batch training loss: 1.286665678024292\n",
      "Epoch validation loss: 1.589464020729065\n",
      "Epoch: 27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 107/107 [00:21<00:00,  5.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average epoch training loss: 1.2667390275224346\n",
      "Last batch training loss: 1.2584857940673828\n",
      "Epoch validation loss: 1.5821871757507324\n",
      "Epoch: 28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 107/107 [00:21<00:00,  5.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average epoch training loss: 1.2498530182883003\n",
      "Last batch training loss: 1.2928460836410522\n",
      "Epoch validation loss: 1.581800389289856\n",
      "Epoch: 29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 107/107 [00:21<00:00,  4.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average epoch training loss: 1.234554877905088\n",
      "Last batch training loss: 1.2436082363128662\n",
      "Epoch validation loss: 1.577298104763031\n",
      "Epoch: 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 107/107 [00:21<00:00,  5.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average epoch training loss: 1.2190044126778006\n",
      "Last batch training loss: 1.1904098987579346\n",
      "Epoch validation loss: 1.5823105454444886\n",
      "Epoch: 31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 107/107 [00:21<00:00,  4.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average epoch training loss: 1.2035604247422975\n",
      "Last batch training loss: 1.2013874053955078\n",
      "Epoch validation loss: 1.5917557954788208\n",
      "Epoch: 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 107/107 [00:21<00:00,  4.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average epoch training loss: 1.1885405756602778\n",
      "Last batch training loss: 1.1575632095336914\n",
      "Epoch validation loss: 1.6000290870666505\n",
      "Epoch: 33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 107/107 [00:21<00:00,  5.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average epoch training loss: 1.176967635332981\n",
      "Last batch training loss: 1.2055195569992065\n",
      "Epoch validation loss: 1.5867462873458862\n",
      "Epoch: 34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 107/107 [00:21<00:00,  4.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average epoch training loss: 1.1612085672182457\n",
      "Last batch training loss: 1.1793259382247925\n",
      "Epoch validation loss: 1.5880399346351624\n",
      "Epoch: 35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 107/107 [00:21<00:00,  4.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average epoch training loss: 1.1463404637630854\n",
      "Last batch training loss: 1.177831768989563\n",
      "Epoch validation loss: 1.5902249097824097\n",
      "Epoch: 36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 107/107 [00:21<00:00,  4.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average epoch training loss: 1.1334810034136906\n",
      "Last batch training loss: 1.178155541419983\n",
      "Epoch validation loss: 1.6176453351974487\n",
      "Epoch: 37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 107/107 [00:21<00:00,  4.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average epoch training loss: 1.119857259999926\n",
      "Last batch training loss: 1.1363736391067505\n",
      "Epoch validation loss: 1.6042938590049745\n",
      "Epoch: 38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 107/107 [00:21<00:00,  4.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average epoch training loss: 1.1052395762684188\n",
      "Last batch training loss: 1.1142548322677612\n",
      "Epoch validation loss: 1.6196529746055603\n",
      "Epoch: 39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 107/107 [00:22<00:00,  4.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average epoch training loss: 1.0954942279886977\n",
      "Last batch training loss: 1.0758405923843384\n",
      "Epoch validation loss: 1.6311343550682067\n",
      "Epoch: 40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 107/107 [00:21<00:00,  5.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average epoch training loss: 1.0820913247973005\n",
      "Last batch training loss: 1.0827819108963013\n",
      "Epoch validation loss: 1.6220815896987915\n",
      "Epoch: 41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 107/107 [00:21<00:00,  5.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average epoch training loss: 1.0676533618820048\n",
      "Last batch training loss: 1.0470728874206543\n",
      "Epoch validation loss: 1.6483017086982727\n",
      "Epoch: 42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 107/107 [00:22<00:00,  4.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average epoch training loss: 1.057369582563917\n",
      "Last batch training loss: 1.0569454431533813\n",
      "Epoch validation loss: 1.646824264526367\n",
      "Epoch: 43\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 107/107 [00:22<00:00,  4.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average epoch training loss: 1.0416490819966682\n",
      "Last batch training loss: 1.059358835220337\n",
      "Epoch validation loss: 1.656047785282135\n",
      "Epoch: 44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 107/107 [00:21<00:00,  4.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average epoch training loss: 1.0287096349992484\n",
      "Last batch training loss: 0.9842891693115234\n",
      "Epoch validation loss: 1.6568758845329286\n",
      "Epoch: 45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 107/107 [00:21<00:00,  5.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average epoch training loss: 1.0167588881243055\n",
      "Last batch training loss: 1.037386417388916\n",
      "Epoch validation loss: 1.6532402515411377\n",
      "Epoch: 46\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 107/107 [00:22<00:00,  4.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average epoch training loss: 1.0047794285221634\n",
      "Last batch training loss: 1.039312720298767\n",
      "Epoch validation loss: 1.6715827345848084\n",
      "Epoch: 47\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 107/107 [00:21<00:00,  5.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average epoch training loss: 0.9949499858874027\n",
      "Last batch training loss: 1.0006977319717407\n",
      "Epoch validation loss: 1.673180067539215\n",
      "Epoch: 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 107/107 [00:22<00:00,  4.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average epoch training loss: 0.9795880869170216\n",
      "Last batch training loss: 1.0079280138015747\n",
      "Epoch validation loss: 1.700710701942444\n",
      "Epoch: 49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 107/107 [00:21<00:00,  4.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average epoch training loss: 0.9691606559486032\n",
      "Last batch training loss: 0.9730602502822876\n",
      "Epoch validation loss: 1.6907180190086364\n",
      "Epoch: 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 107/107 [00:22<00:00,  4.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average epoch training loss: 0.9598854412542326\n",
      "Last batch training loss: 1.0286651849746704\n",
      "Epoch validation loss: 1.7168082237243651\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "sequence_length = 250\n",
    "batch_size = 32\n",
    "dataset_train = DatasetDialogs('dataset_text/dialogs_train.txt', sequence_length)\n",
    "dataset_test = DatasetDialogs('dataset_text/dialogs_test.txt', sequence_length)\n",
    "dataloader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=batch_size, shuffle=True)\n",
    "vocab_size = dataset_train.tokenizer.get_vocab_size()\n",
    "\n",
    "d_model = 512\n",
    "num_layers = 6\n",
    "num_heads = 8\n",
    "d_ff = 2048\n",
    "dropout = 0.1\n",
    "max_seq_length = sequence_length\n",
    "# model = TransformerDecoder(vocab_size, d_model, num_layers, num_heads, d_ff, dropout, max_seq_length)\n",
    "model = TransformerDecoder(vocab_size, d_model, max_seq_length, num_layers, d_ff, num_heads, dropout=0.1)\n",
    "model.to(device)\n",
    "\n",
    "tgt_mask = (1 - torch.triu(\n",
    "  torch.ones(1, sequence_length, sequence_length), diagonal=1)\n",
    ").bool()\n",
    "\n",
    "def init_weights(module):\n",
    "    if isinstance(module, (nn.Linear)):\n",
    "        init.kaiming_normal_(module.weight, mode='fan_in', nonlinearity='relu')\n",
    "        if module.bias is not None:\n",
    "            init.zeros_(module.bias)\n",
    "model.apply(init_weights)\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=1e-4)\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=0)\n",
    "n_epochs = 50\n",
    "n_batches = int(dataset_train.__len__() // batch_size)\n",
    "\n",
    "print(\"Starting model training...\")\n",
    "for epoch in range(n_epochs):\n",
    "    print(f\"Epoch: {epoch + 1}\")\n",
    "    avg_loss = 0\n",
    "    model.train()\n",
    "    for batch_idx, batch in enumerate(tqdm(dataloader_train, total=n_batches)):\n",
    "        x, y = batch\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        outputs = model(x, tgt_mask.to(device))\n",
    "        loss = loss_fn(outputs.view(-1, vocab_size), y.view(-1))\n",
    "        avg_loss += loss.item()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    torch.save(model.state_dict(), f'model_checkpoints/model_checkpoint_{epoch+1}.pth')\n",
    "\n",
    "    avg_loss /= (batch_idx + 1)\n",
    "    print(f\"Average epoch training loss: {avg_loss}\")\n",
    "    print(f\"Last batch training loss: {loss}\")\n",
    "\n",
    "    model.eval()\n",
    "    avg_loss = 0\n",
    "    for batch_idx, batch in enumerate(dataloader_test):\n",
    "        x, y = batch\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        outputs = model(x, tgt_mask.to(device))\n",
    "        loss = loss_fn(outputs.view(-1, vocab_size), y.view(-1))\n",
    "        avg_loss += loss.item()\n",
    "    \n",
    "    avg_loss /= (batch_idx + 1)\n",
    "    print(f\"Epoch validation loss: {avg_loss}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ef8fddb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequence_to_length(sequence, sequence_length, pad_token_idx):\n",
    "    \"\"\"\n",
    "    Completa a sequência com tokens de padding até atingir o comprimento desejado.\n",
    "\n",
    "    Args:\n",
    "        sequence (list): Sequência de tokens (índices) a ser completada.\n",
    "        sequence_length (int): Comprimento desejado da sequência.\n",
    "        pad_token_idx (int): Índice do token de padding.\n",
    "\n",
    "    Returns:\n",
    "        list: Sequência completada com tokens de padding.\n",
    "    \"\"\"\n",
    "    if len(sequence) < sequence_length:\n",
    "        # Adiciona tokens de padding no final da sequência\n",
    "        sequence += [pad_token_idx] * (sequence_length - len(sequence))\n",
    "    elif len(sequence) > sequence_length:\n",
    "        # Trunca a sequência se for maior que o comprimento desejado\n",
    "        sequence = sequence[:sequence_length]\n",
    "    return sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "48ba3675",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def predict(start_text, model, tokenizer, sequence_length=250, temperature=1.0):\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    # Tokeniza e preenche a sequência\n",
    "    sequence = [tokenizer.sos_token_idx()] + [tokenizer.encode(c) for c in start_text]\n",
    "    sequence = pad_sequence_to_length(sequence, sequence_length, tokenizer.pad_token_idx())\n",
    "    input_tokens = torch.tensor(sequence, dtype=torch.long).unsqueeze(0).to(device)\n",
    "\n",
    "    current_text = start_text\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(start_text), sequence_length):\n",
    "            tgt_mask = torch.tril(torch.ones(sequence_length, sequence_length)).to(device).bool()\n",
    "            outputs = model(input_tokens, tgt_mask=tgt_mask)\n",
    "            log_probs = outputs[0, i - 1] / temperature\n",
    "\n",
    "            predicted_token_idx = torch.distributions.Categorical(logits=log_probs).sample().item()\n",
    "\n",
    "            if predicted_token_idx == tokenizer.eos_token_idx():\n",
    "                break\n",
    "\n",
    "            current_text += tokenizer.decode(predicted_token_idx)\n",
    "            input_tokens[0, i] = predicted_token_idx\n",
    "\n",
    "    print('Texto predito:', current_text)\n",
    "    return current_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c73145b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DatasetDialogs('dataset_text/dialogs.txt', 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0fc8f8aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texto predito: Hello , i went.\tit sure isn't. because i was nice to do.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Hello , i went.\\tit sure isn't. because i was nice to do.\""
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict('Hello ', model=model, tokenizer=dataset.tokenizer, temperature=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e08dc889",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texto predito: How are you doing? \ti was hoping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'How are you doing? \\ti was hoping.'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict('How are you doing? ', model=model, tokenizer=dataset.tokenizer, temperature=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327c1ffe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
