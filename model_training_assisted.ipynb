{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.init as init\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prep 1.: Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputEmbeddings(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.d_model = d_model\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Multiplicação com math.sqrt(d_model) pois Positional Encoder tem valores iniciais entre -1 e 1 devido a sin e cos.\n",
    "        # Essa multiplicação escala os valores da inicialização do nn.Embedding para próximo da escala do Positional Encoder.\n",
    "        # Inicialização do nn.Embedding é normal com média 0 e standard deviation embedding_dim ** -0.5\n",
    "        return self.embedding(x) * math.sqrt(self.d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prep 2.: Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_length):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros((max_seq_length, d_model))\n",
    "        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1.: Attention Mechanism Layer (MultiHeadAttention Layer) + Feed Forward Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**bias=False explanation**\n",
    "\n",
    "For certain types of layers, such as transformers and convolutional layers, including a bias term is unnecessary and adds unnecessary overhead to the model.\n",
    "\n",
    "The reason for this is that these layers are typically followed by a normalization layer, such as Batch Normalization or Layer Normalization. These normalization layers center the data at mean=0 (and std=1), effectively removing any bias.\n",
    "\n",
    "Therefore, it is common practice to omit the bias term in transformers and convolutional layers that are preceded by a normalization layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        self.head_dim = d_model // num_heads\n",
    "\n",
    "        self.query_linear = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.key_linear = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.value_linear = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.output_linear = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "        # Entra x com shape (batch_size, seq_length, d_model)\n",
    "        seq_length = x.size(1)\n",
    "        x = x.reshape(batch_size, seq_length, self.num_heads, self.head_dim)\n",
    "        # Sai y com shape (batch_size, num_heads, seq_length, head_dim)\n",
    "        return x.permute(0, 2, 1, 3)\n",
    "    \n",
    "    def compute_attention(self, query, key, value, mask=None):\n",
    "        # Shape de query, key, value (batch_size, num_heads, seq_length, head_dim)\n",
    "        scores = torch.matmul(query, key.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "        attention_weights = F.softmax(scores, dim=-1) # dim -1 significa softmax computada ao longo da dimensão head_dim, que é um chunk de embedding.\n",
    "        return torch.matmul(attention_weights, value)\n",
    "    \n",
    "    def combine_heads(self, x, batch_size):\n",
    "        seq_length = x.size(2)\n",
    "        x = x.permute(0, 2, 1, 3).contiguous()\n",
    "        return x.reshape(batch_size, seq_length, self.d_model)\n",
    "    \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        batch_size = query.size(0)\n",
    "\n",
    "        query = self.split_heads(self.query_linear(query), batch_size)\n",
    "        key = self.split_heads(self.key_linear(key), batch_size)\n",
    "        value = self.split_heads(self.value_linear(value), batch_size)\n",
    "        \n",
    "        attention_weights = self.compute_attention(query, key, value, mask)\n",
    "        output = self.combine_heads(attention_weights, batch_size)\n",
    "        return self.output_linear(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardSubLayer(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.relu(self.fc1(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2.: Transformer Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ff_sublayer = FeedForwardSubLayer(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, src_mask):\n",
    "        attn_output = self.self_attn(x, x, x, mask=src_mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        ff_output = self.ff_sublayer(x)\n",
    "        x = self.norm2(x + self.dropout(ff_output))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, n_layers, num_heads, d_ff, dropout, max_seq_length):\n",
    "        super().__init__()\n",
    "        self.embedding = InputEmbeddings(vocab_size=vocab_size, d_model=d_model)\n",
    "        self.positional_encoder = PositionalEncoding(d_model=d_model, max_seq_length=max_seq_length)\n",
    "        self.encoder_blocks = nn.ModuleList(\n",
    "            [\n",
    "                EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(n_layers) # noqa E501\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def forward(self, x, src_mask):\n",
    "        x = self.embedding(x)\n",
    "        x = self.positional_encoder(x)\n",
    "        for layer in self.encoder_blocks:\n",
    "            x = layer(x, src_mask)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifierHead(nn.Module):\n",
    "    def __init__(self, d_model, num_classes):\n",
    "        super().__init__()\n",
    "        self.classifier_nn = nn.Linear(d_model, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.classifier_nn(x)\n",
    "        return F.softmax(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 256\n",
    "d_model = 512\n",
    "num_layers = 6\n",
    "num_heads = 8\n",
    "d_ff = 2048\n",
    "dropout = 0.2\n",
    "seq_length = 256\n",
    "num_classes = 2\n",
    "\n",
    "transformer_encoder = TransformerEncoder(vocab_size, d_model, num_layers, num_heads, d_ff, dropout, seq_length)\n",
    "classifier = ClassifierHead(d_model, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3.: Transformer Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         ...,\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.]]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones(1, seq_length, seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [0., 0., 1.,  ..., 1., 1., 1.],\n",
       "         [0., 0., 0.,  ..., 1., 1., 1.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 1., 1.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 1.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.triu(torch.ones(1, seq_length, seq_length), diagonal=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tgt_mask = (1 - torch.triu(\n",
    "  torch.ones(1, seq_length, seq_length), diagonal=1)\n",
    ").bool()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ff_sublayer = FeedForwardSubLayer(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, tgt_mask):\n",
    "        attn_output = self.self_attn(x, x, x, tgt_mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        ff_output = self.ff_sublayer(x)\n",
    "        x = self.norm2(x + self.dropout(ff_output))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, num_layers, num_heads, d_ff, dropout, max_seq_length):\n",
    "        super(TransformerDecoder, self).__init__()\n",
    "        self.embedding = InputEmbeddings(vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, max_seq_length)\n",
    "        self.layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "        self.fc = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x, tgt_mask):\n",
    "        x = self.embedding(x)\n",
    "        x = self.positional_encoding(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, tgt_mask)\n",
    "        x = self.fc(x)\n",
    "        # Log de probabilidades para computação mais rápida e maior estabilidade com probas perto de 0.\n",
    "        # Mapeia [0, 1] para (-inf, 0]\n",
    "        return F.log_softmax(x, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 22, 777, 568, 184, 616, 880,  42, 991, 208, 305, 118, 906,  84, 667,\n",
       "         430, 836, 281, 645, 465, 830, 586, 511,  30, 717, 543, 619, 362, 336,\n",
       "         255, 567,  31, 744, 563, 181, 839, 557, 457, 191, 556, 584, 777, 965,\n",
       "          14, 137, 818, 118, 381, 491, 702,  90, 329, 188, 637,  93, 996,  59,\n",
       "           6, 636,  63, 721, 160, 402, 737, 304, 677, 294, 656, 291, 594, 507,\n",
       "         873, 823, 630, 938, 972, 955, 265, 762, 259, 736, 971, 596, 651, 132,\n",
       "         640, 381, 981, 825,  92, 164, 563, 361, 925,  31, 334, 169, 316, 239,\n",
       "         346, 948, 297, 643, 217, 422, 743,  45, 434, 812, 153, 262, 864, 900,\n",
       "         677, 329, 927, 381, 927, 331, 416, 418, 837, 214, 898, 255, 227, 829,\n",
       "         445, 573, 511, 414, 716, 867,   5, 431, 539, 727, 671, 678,  72, 548,\n",
       "          67, 266, 317, 507, 776, 113, 566,  27, 869, 749,  67,  89, 995, 788,\n",
       "         383, 160, 862, 426, 668, 906, 831, 435, 838, 992,  49, 525, 822, 509,\n",
       "         191, 813, 809, 662, 533, 310, 135, 276, 787, 687, 511, 989,  64, 893,\n",
       "         484, 484, 863, 249, 401,  38,  51, 895, 752, 383,  19,   8,  64, 665,\n",
       "         910, 533, 821, 945, 155, 227, 830, 253, 593,  22, 223, 242, 421, 767,\n",
       "         258, 298, 389, 787,  97, 744, 299, 651, 899, 544, 934,   1,  77, 294,\n",
       "         829, 917, 988, 819, 584, 509, 278, 102, 524, 383, 946, 143,  42, 717,\n",
       "         256, 271, 589, 513, 768, 466, 231, 107, 205, 149, 293, 741,  41, 246,\n",
       "         531, 728,  62, 206]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_seq_length = 256\n",
    "batch_size = 1\n",
    "vocab_size = 1000\n",
    "torch.randint(low=0, high=vocab_size, size=(batch_size, max_seq_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 256\n",
    "batch_size = 2\n",
    "vocab_size = 1000\n",
    "input_tokens = torch.randint(low=0, high=vocab_size, size=(batch_size, max_seq_length))\n",
    "\n",
    "transformer_decoder = TransformerDecoder(vocab_size, d_model, num_layers, num_heads, d_ff, dropout, max_seq_length)   \n",
    "output = transformer_decoder(input_tokens, tgt_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 57, 145, 314,  78, 183, 113, 407, 586, 535, 885, 925, 587, 816,  37,\n",
      "          55, 252,  66, 988, 593, 906, 248, 802, 770, 274, 645, 344, 320, 297,\n",
      "         471, 294, 371, 241, 347, 304, 181,  50, 636,  22, 489, 439, 534, 588,\n",
      "          26, 190, 113, 788, 138,  73, 686, 332, 215, 204, 586, 198, 443, 395,\n",
      "         999, 216, 793, 212, 881, 363, 592,  90, 370,  33,  51, 638, 483, 558,\n",
      "         729, 349, 886, 729,  34, 883, 652, 656, 170, 521, 276, 407, 695, 803,\n",
      "          12, 893, 641, 612, 315, 483, 400, 611, 555, 293, 858, 158, 923, 402,\n",
      "         614, 860, 409,  37, 564, 998,  22, 337,  60, 146, 728, 210, 684, 342,\n",
      "         227, 722, 147, 990,  95, 418, 666, 917, 644, 440, 611, 803, 626, 298,\n",
      "         990, 534, 937, 691,  16, 659, 798,  46, 199, 301, 503, 525, 418, 209,\n",
      "         985, 981, 292, 457, 605,  72, 953, 105, 937, 685, 472, 343, 659, 150,\n",
      "         306,   7, 350, 460, 349, 602, 314, 822, 474,  57, 295, 274, 142, 213,\n",
      "         719, 272, 287, 497, 436, 992,  90, 989, 272, 447, 446, 142, 920, 200,\n",
      "          73, 596, 159, 438, 761, 771,  97, 404, 401, 976, 823,  94, 247, 588,\n",
      "         714, 571,   9, 418,  30, 817, 911,  68, 325, 992, 421, 797, 917, 617,\n",
      "         454, 197, 816, 715, 881, 308,  46, 116, 929, 752,  26, 885, 510, 764,\n",
      "         298, 412, 948, 275, 744, 810, 135, 408, 443, 898, 770,  64, 252, 694,\n",
      "         125, 740,  59, 714, 184, 344, 588, 112, 338, 390, 350,  22, 247, 347,\n",
      "         916, 139, 474, 844],\n",
      "        [329, 263,  30, 438, 722, 602, 822, 300, 457, 184, 995, 911, 377, 450,\n",
      "         209, 140, 112, 379, 685, 956, 806, 205, 824, 924, 849, 360, 470, 825,\n",
      "         720, 179, 212, 668, 845, 353, 888, 834, 293, 599, 663, 333,  54, 504,\n",
      "         702, 671, 613, 564, 984, 912, 227, 169, 407, 608, 259, 258, 737, 693,\n",
      "         376, 941, 156, 951, 763, 127,  28,  32, 243, 804, 296, 928, 247, 203,\n",
      "         124, 921, 509, 706, 227,   1, 264, 574, 269, 953, 412, 929,  18, 362,\n",
      "         162, 263, 290,  86, 550, 542,  68, 552, 383, 748, 197, 109, 622, 278,\n",
      "          95, 876, 748, 858, 921, 725, 479, 843, 471, 561, 408, 389, 165, 198,\n",
      "         351, 293, 890, 836, 575, 922, 265, 651, 860, 533, 754, 709, 492, 114,\n",
      "         195, 789,  38, 907, 817, 948, 383, 470, 143, 548,  40, 829, 248, 967,\n",
      "         593, 436, 419,  47, 724, 686, 740, 769, 917, 236, 565, 445, 641, 598,\n",
      "         317, 194,  47, 196, 420, 850, 269, 123, 868, 961, 221, 151, 614, 140,\n",
      "         916, 650, 336, 326, 406, 595, 131, 386, 989, 832, 577,  33, 518, 714,\n",
      "           6, 793, 465, 680, 473, 881, 840, 481, 365,  55, 214, 998,  26, 191,\n",
      "         643, 786, 942, 129, 135, 627, 261, 572, 852,  16, 904, 787, 362,  54,\n",
      "         495, 521, 987, 438, 513, 866,  53, 563, 584, 198, 882, 619,  51, 601,\n",
      "         658, 386, 633, 568, 302, 639, 214,  45,  72, 335, 704, 390, 191, 214,\n",
      "         787, 290, 332,  86, 779, 932,  92, 709, 484, 860, 834, 258,  10, 986,\n",
      "         655, 656, 690, 681]])\n"
     ]
    }
   ],
   "source": [
    "print(input_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-6.7642, -6.8077, -7.8105,  ..., -7.3003, -7.4120, -7.1858],\n",
      "         [-7.0911, -7.6265, -7.7117,  ..., -6.6925, -6.7127, -7.6584],\n",
      "         [-7.0995, -7.4291, -6.8647,  ..., -7.2756, -7.0006, -7.9697],\n",
      "         ...,\n",
      "         [-7.1360, -6.5465, -7.3965,  ..., -6.7170, -7.5379, -7.3314],\n",
      "         [-6.5048, -7.2928, -7.6898,  ..., -6.2444, -7.6415, -7.2333],\n",
      "         [-7.2406, -7.5268, -7.3894,  ..., -6.7142, -7.4443, -8.0571]],\n",
      "\n",
      "        [[-5.9346, -6.9170, -6.5690,  ..., -7.1437, -6.1088, -7.4729],\n",
      "         [-6.5090, -6.5014, -6.1054,  ..., -7.6661, -7.3899, -8.0944],\n",
      "         [-8.3925, -7.2747, -6.2694,  ..., -7.6565, -7.4415, -7.6147],\n",
      "         ...,\n",
      "         [-7.3333, -7.1162, -7.4921,  ..., -7.1177, -7.4886, -7.1390],\n",
      "         [-6.5008, -6.4315, -6.2267,  ..., -6.7472, -7.8710, -7.2734],\n",
      "         [-6.8413, -6.8020, -7.1416,  ..., -6.7609, -7.6003, -6.8107]]],\n",
      "       grad_fn=<LogSoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.randn(size=(2, 256, 8, 64)).view((2, 256, 512)) # for debugging reshapes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 4.: Encoder-decoder Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ff_sublayer = FeedForwardSubLayer(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, y, tgt_mask, cross_mask):\n",
    "        self_attn_output = self.self_attn(x, x, x, tgt_mask)\n",
    "        x = self.norm1(x + self.dropout(self_attn_output))\n",
    "        cross_attn_output = self.cross_attn(x, y, y, cross_mask)\n",
    "        x = self.norm2(x + self.dropout(cross_attn_output))\n",
    "        ff_output = self.ff_sublayer(x)\n",
    "        x = self.norm3(x + self.dropout(ff_output))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, num_layers, num_heads, d_ff, dropout, max_seq_length):\n",
    "        super(TransformerDecoder, self).__init__()\n",
    "        self.embedding = InputEmbeddings(vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, max_seq_length)\n",
    "        self.layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "        self.fc = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x, y, tgt_mask, cross_mask):\n",
    "        x = self.embedding(x)\n",
    "        x = self.positional_encoding(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, y, tgt_mask, cross_mask)\n",
    "        x = self.fc(x)\n",
    "        # Log de probabilidades para computação mais rápida e maior estabilidade com probas perto de 0.\n",
    "        # Mapeia [0, 1] para (-inf, 0]\n",
    "        return F.log_softmax(x, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout):\n",
    "        super().__init__()\n",
    "        self.encoder = TransformerEncoder(vocab_size, d_model, num_layers, num_heads, d_ff, dropout, max_seq_length)\n",
    "        self.decoder = TransformerDecoder(vocab_size, d_model, num_layers, num_heads, d_ff, dropout, max_seq_length)\n",
    "\n",
    "    def forward(self, x, src_mask, tgt_mask, cross_mask):\n",
    "        encoder_output = self.encoder(x, src_mask)\n",
    "        decoder_output = self.decoder(x, encoder_output, tgt_mask, cross_mask)\n",
    "        return decoder_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_padding_mask(sequence, pad_token=0):\n",
    "    # Mask out padding tokens (assumes pad_token is 0)\n",
    "    return (sequence != pad_token).unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "# sequence = torch.tensor([2, 6, 30, 120, 0, 0, 0, 0])\n",
    "src_mask = generate_padding_mask(input_tokens)\n",
    "cross_mask = src_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-7.2952, -7.7172, -6.4120,  ..., -8.0577, -6.7412, -6.9631],\n",
      "         [-8.0978, -5.6856, -6.9766,  ..., -7.3987, -5.9954, -7.1549],\n",
      "         [-6.8227, -6.9482, -6.8970,  ..., -6.4872, -6.5135, -6.8972],\n",
      "         ...,\n",
      "         [-7.2849, -7.2086, -6.8851,  ..., -8.2023, -7.0849, -7.5703],\n",
      "         [-6.5633, -6.9625, -7.1137,  ..., -7.2809, -6.3619, -6.8622],\n",
      "         [-7.9815, -6.7311, -6.9914,  ..., -7.6957, -5.9318, -7.9336]],\n",
      "\n",
      "        [[-7.9445, -7.5784, -7.1699,  ..., -7.4563, -7.4804, -7.3523],\n",
      "         [-8.1715, -6.8377, -6.5618,  ..., -6.8760, -6.6384, -5.8102],\n",
      "         [-8.4128, -7.2745, -7.4224,  ..., -7.3137, -6.4476, -6.8861],\n",
      "         ...,\n",
      "         [-6.9692, -6.8992, -7.4177,  ..., -7.3548, -7.0166, -6.5395],\n",
      "         [-6.2935, -7.0444, -7.1079,  ..., -7.5451, -6.3904, -7.3940],\n",
      "         [-7.0938, -7.0121, -7.0166,  ..., -7.9626, -6.6922, -8.1125]]],\n",
      "       grad_fn=<LogSoftmaxBackward0>)\n",
      "torch.Size([2, 256, 1000])\n"
     ]
    }
   ],
   "source": [
    "transformer = Transformer(vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout)\n",
    "outputs = transformer(input_tokens, src_mask, tgt_mask, cross_mask)\n",
    "print(outputs)\n",
    "print(outputs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 259 tokens possíveis:\n",
    "# ASCII + <UNK> + <SOS> + <EOS> + <PAD>\n",
    "\n",
    "class TokenizerChar:\n",
    "    def __init__(self):\n",
    "        self.chr_to_idx = {chr(v): v for v in range(1, 257)}\n",
    "        self.chr_to_idx['<SOS>'] = 257\n",
    "        self.chr_to_idx['<EOS>'] = 258\n",
    "        self.chr_to_idx['<PAD>'] = 0\n",
    "        self.chr_to_idx['<UNK>'] = 259\n",
    "\n",
    "        self.idx_to_chr = {v: k for k, v in self.chr_to_idx.items()}\n",
    "\n",
    "        self.vocab_size = len(self.chr_to_idx.keys())\n",
    "\n",
    "    def encode(self, char):\n",
    "        if char in self.chr_to_idx.keys():\n",
    "            return self.chr_to_idx[char]\n",
    "        else:\n",
    "            return 259\n",
    "    \n",
    "    def decode(self, token_idx):\n",
    "        return self.idx_to_chr[token_idx]\n",
    "    \n",
    "    def sos_token(self):\n",
    "        return '<SOS>'\n",
    "    \n",
    "    def sos_token_idx(self):\n",
    "        return self.chr_to_idx['<SOS>']\n",
    "\n",
    "    def eos_token(self):\n",
    "        return '<EOS>'\n",
    "    \n",
    "    def eos_token_idx(self):\n",
    "        return self.chr_to_idx['<EOS>']\n",
    "    \n",
    "    def pad_token(self):\n",
    "        return '<PAD>'\n",
    "    \n",
    "    def pad_token_idx(self):\n",
    "        return self.chr_to_idx['<PAD>']\n",
    "    \n",
    "    def get_vocab_size(self):\n",
    "        return self.vocab_size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3725\n"
     ]
    }
   ],
   "source": [
    "with open('dataset_text/dialogs.txt') as file:\n",
    "    print(len(file.read().split('\\n')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista = [1, 2, 3, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lista[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3, 4]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lista[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class DatasetDialogs(Dataset):\n",
    "    def __init__(self, dataset_path, sentence_length):\n",
    "        self.dataset_path = dataset_path\n",
    "        self.sentence_length = sentence_length\n",
    "        self.tokenizer = TokenizerChar()\n",
    "\n",
    "    def __len__(self):\n",
    "        with open(self.dataset_path, 'r') as dataset:\n",
    "            num_of_sentences = len(dataset.read().split('\\n'))\n",
    "        return num_of_sentences\n",
    "    \n",
    "    def get_shape(self):\n",
    "        with open(self.dataset_path, 'r') as dataset:\n",
    "            num_of_sentences = len(dataset.read().split('\\n'))\n",
    "        return (num_of_sentences, self.sentence_length)\n",
    "\n",
    "    def __getitem__(self, line_idx):\n",
    "        with open(self.dataset_path, 'r') as dataset:\n",
    "            selected_sentence = dataset.read().split('\\n')[line_idx]\n",
    "            if len(selected_sentence) < self.sentence_length:\n",
    "                input_tokens = [self.tokenizer.sos_token_idx()] + [self.tokenizer.encode(char) for char in selected_sentence]\n",
    "                pad_length = self.sentence_length - len(input_tokens)\n",
    "                pad_tokens = [self.tokenizer.pad_token_idx()] * pad_length\n",
    "                input_tokens += pad_tokens\n",
    "                input_tokens.append(self.tokenizer.eos_token_idx())\n",
    "            else:\n",
    "                selected_sentence = selected_sentence[:self.sentence_length - 1]\n",
    "                input_tokens = [self.tokenizer.sos_token_idx()] + [self.tokenizer.encode(char) for char in selected_sentence]\n",
    "                input_tokens[self.sentence_length - 1] = self.tokenizer.eos_token_idx()\n",
    "            try:\n",
    "                x = torch.tensor(input_tokens[:-1])\n",
    "                y = torch.tensor(input_tokens[1:])\n",
    "            except RuntimeError as e:\n",
    "                print(e)\n",
    "                print(f\"Input tokens: {input_tokens}\")\n",
    "                raise e\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DatasetDialogs('dataset_text/dialogs.txt', 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'h'"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chr(104)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([257, 104, 105,  44,  32, 104, 111, 119,  32,  97, 114, 101,  32, 121,\n",
       "         111, 117,  32, 100, 111, 105, 110, 103,  63,   9, 105,  39, 109,  32,\n",
       "         102, 105, 110, 101,  46,  32, 104, 111, 119,  32,  97,  98, 111, 117,\n",
       "         116,  32, 121, 111, 117, 114, 115, 101, 108, 102,  63,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0]),\n",
       " tensor([104, 105,  44,  32, 104, 111, 119,  32,  97, 114, 101,  32, 121, 111,\n",
       "         117,  32, 100, 111, 105, 110, 103,  63,   9, 105,  39, 109,  32, 102,\n",
       "         105, 110, 101,  46,  32, 104, 111, 119,  32,  97,  98, 111, 117, 116,\n",
       "          32, 121, 111, 117, 114, 115, 101, 108, 102,  63,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0, 258]))"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting model training...\n",
      "Epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 8/107 [00:15<03:09,  1.92s/it]"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "sequence_length = 150\n",
    "batch_size = 32\n",
    "dataset_train = DatasetDialogs('dataset_text/dialogs_train.txt', sequence_length)\n",
    "dataset_test = DatasetDialogs('dataset_text/dialogs_test.txt', sequence_length)\n",
    "dataloader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=batch_size, shuffle=True)\n",
    "vocab_size = dataset_train.tokenizer.get_vocab_size()\n",
    "\n",
    "d_model = 512\n",
    "num_layers = 6\n",
    "num_heads = 8\n",
    "d_ff = 2048\n",
    "dropout = 0.1\n",
    "max_seq_length = sequence_length\n",
    "model = TransformerDecoder(vocab_size, d_model, num_layers, num_heads, d_ff, dropout, max_seq_length)\n",
    "\n",
    "tgt_mask = (1 - torch.triu(\n",
    "  torch.ones(1, sequence_length, sequence_length), diagonal=1)\n",
    ").bool()\n",
    "\n",
    "def init_weights(module):\n",
    "    if isinstance(module, (nn.Linear)):\n",
    "        init.kaiming_normal_(module.weight, mode='fan_in', nonlinearity='relu')\n",
    "        if module.bias is not None:\n",
    "            init.zeros_(module.bias)\n",
    "model.apply(init_weights)\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=1e-4)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "n_epochs = 2\n",
    "n_batches = int(dataset_train.__len__() // batch_size)\n",
    "\n",
    "print(\"Starting model training...\")\n",
    "for epoch in range(n_epochs):\n",
    "    print(f\"Epoch: {epoch + 1}\")\n",
    "    avg_loss = 0\n",
    "    model.train()\n",
    "    for batch_idx, batch in enumerate(tqdm(dataloader_train, total=n_batches)):\n",
    "        x, y = batch\n",
    "        outputs = model(x, tgt_mask)\n",
    "        loss = loss_fn(outputs.view(-1, vocab_size), y.view(-1))\n",
    "        avg_loss += loss.item()\n",
    "        # print(f\"Current training loss: {loss.item()}\")\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    torch.save(model.state_dict(), f'model_checkpoints/model_checkpoint_{epoch+1}.pth')\n",
    "\n",
    "    avg_loss /= (batch_idx + 1)\n",
    "    print(f\"Average epoch training loss: {avg_loss}\")\n",
    "    print(f\"Last batch training loss: {loss}\")\n",
    "\n",
    "    model.eval()\n",
    "    avg_loss = 0\n",
    "    for batch_idx, batch in enumerate(dataloader_test):\n",
    "        x, y = batch\n",
    "        outputs = model(x, tgt_mask)\n",
    "        loss = loss_fn(outputs.view(-1, vocab_size), y.view(-1))\n",
    "        avg_loss += loss.item()\n",
    "    \n",
    "    avg_loss /= (batch_idx + 1)\n",
    "    print(f\"Epoch validation loss: {avg_loss}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Generation / Model Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(start_text):\n",
    "    model.eval()\n",
    "    tokenizer = TokenizerChar()\n",
    "    input_tokens = torch.tensor([tokenizer.chr_to_idx[token] for token in start_text])\n",
    "\n",
    "    current_text = start_text\n",
    "    for _ in range(sequence_length - len(start_text)):\n",
    "        outputs = model(input_tokens, tgt_mask)\n",
    "        outputs_list = outputs.view(-1, vocab_size)\n",
    "        predicted_token_idx = torch.multinomial(outputs_list, num_samples=1)\n",
    "        current_text += tokenizer.decode(predicted_token_idx)\n",
    "\n",
    "    final_text = current_text\n",
    "    print('Texto preditado: ', final_text)\n",
    "    return final_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict('hi, how are you doing?')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
