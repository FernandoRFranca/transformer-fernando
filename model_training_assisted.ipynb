{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prep 1.: Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputEmbeddings(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.d_model = d_model\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Multiplicação com math.sqrt(d_model) pois Positional Encoder tem valores iniciais entre -1 e 1 devido a sin e cos.\n",
    "        # Essa multiplicação escala os valores da inicialização do nn.Embedding para próximo da escala do Positional Encoder.\n",
    "        # Inicialização do nn.Embedding é normal com média 0 e standard deviation embedding_dim ** -0.5\n",
    "        return self.embedding(x) * math.sqrt(self.d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prep 2.: Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_length):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros((max_seq_length, d_model))\n",
    "        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1.: Attention Mechanism Layer (MultiHeadAttention Layer) + Feed Forward Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        self.head_dim = d_model // num_heads\n",
    "\n",
    "        self.query_linear = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.key_linear = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.value_linear = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.output_linear = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "        # Entra x com shape (batch_size, seq_length, d_model)\n",
    "        seq_length = x.size(1)\n",
    "        x = x.reshape(batch_size, seq_length, self.num_heads, self.head_dim)\n",
    "        # Sai y com shape (batch_size, num_heads, seq_length, head_dim)\n",
    "        return x.permute(0, 2, 1, 3)\n",
    "    \n",
    "    def compute_attention(self, query, key, value, mask=None):\n",
    "        # Shape de query, key, value (batch_size, num_heads, seq_length, head_dim)\n",
    "        scores = torch.matmul(query, key.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "        attention_weights = F.softmax(scores, dim=-1) # dim -1 significa softmax computada ao longo da dimensão head_dim, que é um chunk de embedding.\n",
    "        return torch.matmul(attention_weights, value)\n",
    "    \n",
    "    def combine_heads(self, x, batch_size):\n",
    "        seq_length = x.size(2)\n",
    "        x = x.permute(0, 2, 1, 3).contiguous()\n",
    "        return x.reshape(batch_size, seq_length, self.d_model)\n",
    "    \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        batch_size = query.size(0)\n",
    "\n",
    "        query = self.split_heads(self.query_linear(query), batch_size)\n",
    "        key = self.split_heads(self.key_linear(key), batch_size)\n",
    "        value = self.split_heads(self.value_linear(value), batch_size)\n",
    "        \n",
    "        attention_weights = self.compute_attention(query, key, value, mask)\n",
    "        output = self.combine_heads(attention_weights, batch_size)\n",
    "        return self.output_linear(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardSubLayer(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.relu(self.fc1(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2.: Transformer Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ff_sublayer = FeedForwardSubLayer(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, src_mask):\n",
    "        attn_output = self.self_attn(x, x, x, mask=src_mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        ff_output = self.ff_sublayer(x)\n",
    "        x = self.norm2(x + self.dropout(ff_output))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, n_layers, num_heads, d_ff, dropout, max_seq_length):\n",
    "        super().__init__()\n",
    "        self.embedding = InputEmbeddings(vocab_size=vocab_size, d_model=d_model)\n",
    "        self.positional_encoder = PositionalEncoding(d_model=d_model, max_seq_length=max_seq_length)\n",
    "        self.encoder_blocks = nn.ModuleList(\n",
    "            [\n",
    "                EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(n_layers) # noqa E501\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def forward(self, x, src_mask):\n",
    "        x = self.embedding(x)\n",
    "        x = self.positional_encoder(x)\n",
    "        for layer in self.encoder_blocks:\n",
    "            x = layer(x, src_mask)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifierHead(nn.Module):\n",
    "    def __init__(self, d_model, num_classes):\n",
    "        super().__init__()\n",
    "        self.classifier_nn = nn.Linear(d_model, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.classifier_nn(x)\n",
    "        return F.softmax(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 256\n",
    "d_model = 512\n",
    "num_layers = 6\n",
    "num_heads = 8\n",
    "d_ff = 2048\n",
    "dropout = 0.2\n",
    "seq_length = 256\n",
    "num_classes = 2\n",
    "\n",
    "transformer_encoder = TransformerEncoder(vocab_size, d_model, num_layers, num_heads, d_ff, dropout, seq_length)\n",
    "classifier = ClassifierHead(d_model, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3.: Transformer Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         ...,\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.]]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones(1, seq_length, seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [0., 0., 1.,  ..., 1., 1., 1.],\n",
       "         [0., 0., 0.,  ..., 1., 1., 1.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 1., 1.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 1.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.triu(torch.ones(1, seq_length, seq_length), diagonal=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tgt_mask = (1 - torch.triu(\n",
    "  torch.ones(1, seq_length, seq_length), diagonal=1)\n",
    ").bool()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ff_sublayer = FeedForwardSubLayer(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, tgt_mask):\n",
    "        attn_output = self.self_attn(x, x, x, tgt_mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        ff_output = self.ff_sublayer(x)\n",
    "        x = self.norm2(x + self.dropout(ff_output))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, num_layers, num_heads, d_ff, dropout, max_seq_length):\n",
    "        super(TransformerDecoder, self).__init__()\n",
    "        self.embedding = InputEmbeddings(vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, max_seq_length)\n",
    "        self.layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "        self.fc = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x, tgt_mask):\n",
    "        x = self.embedding(x)\n",
    "        x = self.positional_encoding(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, tgt_mask)\n",
    "        x = self.fc(x)\n",
    "        # Log de probabilidades para computação mais rápida e maior estabilidade com probas perto de 0.\n",
    "        # Mapeia [0, 1] para (-inf, 0]\n",
    "        return F.log_softmax(x, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 22, 777, 568, 184, 616, 880,  42, 991, 208, 305, 118, 906,  84, 667,\n",
       "         430, 836, 281, 645, 465, 830, 586, 511,  30, 717, 543, 619, 362, 336,\n",
       "         255, 567,  31, 744, 563, 181, 839, 557, 457, 191, 556, 584, 777, 965,\n",
       "          14, 137, 818, 118, 381, 491, 702,  90, 329, 188, 637,  93, 996,  59,\n",
       "           6, 636,  63, 721, 160, 402, 737, 304, 677, 294, 656, 291, 594, 507,\n",
       "         873, 823, 630, 938, 972, 955, 265, 762, 259, 736, 971, 596, 651, 132,\n",
       "         640, 381, 981, 825,  92, 164, 563, 361, 925,  31, 334, 169, 316, 239,\n",
       "         346, 948, 297, 643, 217, 422, 743,  45, 434, 812, 153, 262, 864, 900,\n",
       "         677, 329, 927, 381, 927, 331, 416, 418, 837, 214, 898, 255, 227, 829,\n",
       "         445, 573, 511, 414, 716, 867,   5, 431, 539, 727, 671, 678,  72, 548,\n",
       "          67, 266, 317, 507, 776, 113, 566,  27, 869, 749,  67,  89, 995, 788,\n",
       "         383, 160, 862, 426, 668, 906, 831, 435, 838, 992,  49, 525, 822, 509,\n",
       "         191, 813, 809, 662, 533, 310, 135, 276, 787, 687, 511, 989,  64, 893,\n",
       "         484, 484, 863, 249, 401,  38,  51, 895, 752, 383,  19,   8,  64, 665,\n",
       "         910, 533, 821, 945, 155, 227, 830, 253, 593,  22, 223, 242, 421, 767,\n",
       "         258, 298, 389, 787,  97, 744, 299, 651, 899, 544, 934,   1,  77, 294,\n",
       "         829, 917, 988, 819, 584, 509, 278, 102, 524, 383, 946, 143,  42, 717,\n",
       "         256, 271, 589, 513, 768, 466, 231, 107, 205, 149, 293, 741,  41, 246,\n",
       "         531, 728,  62, 206]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_seq_length = 256\n",
    "batch_size = 1\n",
    "vocab_size = 1000\n",
    "torch.randint(low=0, high=vocab_size, size=(batch_size, max_seq_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 256\n",
    "batch_size = 2\n",
    "vocab_size = 1000\n",
    "input_tokens = torch.randint(low=0, high=vocab_size, size=(batch_size, max_seq_length))\n",
    "\n",
    "transformer_decoder = TransformerDecoder(vocab_size, d_model, num_layers, num_heads, d_ff, dropout, max_seq_length)   \n",
    "output = transformer_decoder(input_tokens, tgt_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[958, 611, 587, 976, 133, 193, 186, 617, 401, 119, 290,  58, 179, 635,\n",
      "         421,  90, 253, 819, 869, 303, 375, 250, 687, 979, 596, 745, 829, 304,\n",
      "         167, 131, 303, 189, 362, 272, 922, 524, 586,  76,   2, 427, 229, 893,\n",
      "         637, 250, 658, 925, 880, 383, 536, 884, 798, 145, 735, 841, 419, 407,\n",
      "         854, 728, 361, 763, 383, 740, 257, 397, 686, 393, 174, 876, 742, 641,\n",
      "         444, 383, 470, 110,  16,  48, 608, 698, 622, 774, 199, 833, 422, 696,\n",
      "         499, 959, 887, 909, 567, 525, 100, 507, 833, 927, 635, 907, 630, 719,\n",
      "          15, 755, 285, 606, 538,  75, 464, 746, 231, 840, 213, 662, 296, 260,\n",
      "         229, 884, 433, 795, 365, 558, 279, 668, 815, 292, 498, 834, 590, 243,\n",
      "         858, 217, 473, 783, 399, 473, 213, 122, 951, 514, 940, 419,  56,  77,\n",
      "         524, 392, 932, 704, 477, 228, 588, 351, 292, 771, 776, 753, 440, 463,\n",
      "         168,  49, 975, 599, 594, 995, 924, 737, 325, 829, 258, 714, 572, 400,\n",
      "           7, 892, 747, 249, 210, 212, 352, 746, 560, 674,  35, 739, 350, 491,\n",
      "         964, 792, 396, 471, 472, 257, 289, 411, 608, 235, 168,  43, 912, 224,\n",
      "         520, 873, 798, 712,  42, 183, 126,  51, 489, 801, 791, 280, 950, 347,\n",
      "         540, 236, 456, 756, 278,   9, 893, 533, 182, 193, 129, 627, 803, 668,\n",
      "         482, 194, 928, 269, 893, 221, 501, 379, 685,  17,   0, 508, 357, 978,\n",
      "         455, 194, 579, 434, 555,  84, 718, 411, 571, 480, 771, 670, 914, 610,\n",
      "         793, 752, 363,  62],\n",
      "        [462, 668, 328, 262, 195, 160, 825, 326, 494, 909, 139, 787, 242, 669,\n",
      "         949, 781, 658, 951, 954, 972, 529, 241, 350, 503, 493, 545, 452, 972,\n",
      "         156, 454, 605, 824, 685, 125, 521, 905, 705, 145, 286, 303, 484, 267,\n",
      "         645, 647, 633, 772, 113, 718,  41, 548, 864, 514, 725,  72, 358, 894,\n",
      "         180, 840, 574, 204,  57, 360, 483, 444, 247, 669, 912, 655, 600, 195,\n",
      "         895,  82, 485, 551, 604,  61, 471, 709, 340,  33, 702, 401, 431, 222,\n",
      "         607, 125, 323, 508, 281, 702, 236, 195, 554, 969, 802, 524,  89, 907,\n",
      "         550, 628,  69, 736, 951, 689, 366, 980, 567, 145, 569, 909,  45, 950,\n",
      "         551, 456, 177, 693,  82,  34, 951, 832, 772, 956, 901, 189, 543, 848,\n",
      "          82, 487,   6, 736,  91, 801,  43, 490, 962, 875, 215, 407,  55, 781,\n",
      "         243, 710, 969, 989, 767, 574,  97, 679, 761, 360, 518, 964, 369, 922,\n",
      "         459, 991, 402, 484, 199, 563, 479, 650, 726,  81, 372, 253, 210, 540,\n",
      "          68,  98, 266, 238, 760, 326, 542, 834, 548, 706, 214, 244, 223, 829,\n",
      "         702, 396,  35,  64, 695, 600, 887,  13, 987, 733, 436, 772, 584,  75,\n",
      "         623, 884, 768, 103, 101, 953, 513, 344, 826, 844, 656, 851, 523, 991,\n",
      "         810, 794, 885, 725,  16, 963, 743, 793, 784, 394, 488, 900,  79, 956,\n",
      "         692, 624, 867, 793, 242, 978, 707, 148, 830, 350, 861,  79, 168, 934,\n",
      "         809, 662, 131, 372,  66, 733, 629, 549, 704, 138, 847, 999, 756, 105,\n",
      "         138, 332, 698, 616]])\n"
     ]
    }
   ],
   "source": [
    "print(input_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.randn(size=(2, 256, 8, 64)).view((2, 256, 512)) # for debugging reshapes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 4.: Encoder-decoder Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ff_sublayer = FeedForwardSubLayer(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, y, tgt_mask, cross_mask):\n",
    "        self_attn_output = self.self_attn(x, x, x, tgt_mask)\n",
    "        x = self.norm1(x + self.dropout(self_attn_output))\n",
    "        cross_attn_output = self.cross_attn(x, y, y, cross_mask)\n",
    "        x = self.norm2(x + self.dropout(cross_attn_output))\n",
    "        ff_output = self.ff_sublayer(x)\n",
    "        x = self.norm3(x + self.dropout(ff_output))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, num_layers, num_heads, d_ff, dropout, max_seq_length):\n",
    "        super(TransformerDecoder, self).__init__()\n",
    "        self.embedding = InputEmbeddings(vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, max_seq_length)\n",
    "        self.layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "        self.fc = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x, y, tgt_mask, cross_mask):\n",
    "        x = self.embedding(x)\n",
    "        x = self.positional_encoding(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, y, tgt_mask, cross_mask)\n",
    "        x = self.fc(x)\n",
    "        # Log de probabilidades para computação mais rápida e maior estabilidade com probas perto de 0.\n",
    "        # Mapeia [0, 1] para (-inf, 0]\n",
    "        return F.log_softmax(x, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout):\n",
    "        super().__init__()\n",
    "        self.encoder = TransformerEncoder(vocab_size, d_model, num_layers, num_heads, d_ff, dropout, max_seq_length)\n",
    "        self.decoder = TransformerDecoder(vocab_size, d_model, num_layers, num_heads, d_ff, dropout, max_seq_length)\n",
    "\n",
    "    def forward(self, x, src_mask, tgt_mask, cross_mask):\n",
    "        encoder_output = self.encoder(x, src_mask)\n",
    "        decoder_output = self.decoder(x, encoder_output, tgt_mask, cross_mask)\n",
    "        return decoder_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_padding_mask(sequence, pad_token=0):\n",
    "    # Mask out padding tokens (assumes pad_token is 0)\n",
    "    return (sequence != pad_token).unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "# sequence = torch.tensor([2, 6, 30, 120, 0, 0, 0, 0])\n",
    "src_mask = generate_padding_mask(input_tokens)\n",
    "cross_mask = src_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-7.2952, -7.7172, -6.4120,  ..., -8.0577, -6.7412, -6.9631],\n",
      "         [-8.0978, -5.6856, -6.9766,  ..., -7.3987, -5.9954, -7.1549],\n",
      "         [-6.8227, -6.9482, -6.8970,  ..., -6.4872, -6.5135, -6.8972],\n",
      "         ...,\n",
      "         [-7.2849, -7.2086, -6.8851,  ..., -8.2023, -7.0849, -7.5703],\n",
      "         [-6.5633, -6.9625, -7.1137,  ..., -7.2809, -6.3619, -6.8622],\n",
      "         [-7.9815, -6.7311, -6.9914,  ..., -7.6957, -5.9318, -7.9336]],\n",
      "\n",
      "        [[-7.9445, -7.5784, -7.1699,  ..., -7.4563, -7.4804, -7.3523],\n",
      "         [-8.1715, -6.8377, -6.5618,  ..., -6.8760, -6.6384, -5.8102],\n",
      "         [-8.4128, -7.2745, -7.4224,  ..., -7.3137, -6.4476, -6.8861],\n",
      "         ...,\n",
      "         [-6.9692, -6.8992, -7.4177,  ..., -7.3548, -7.0166, -6.5395],\n",
      "         [-6.2935, -7.0444, -7.1079,  ..., -7.5451, -6.3904, -7.3940],\n",
      "         [-7.0938, -7.0121, -7.0166,  ..., -7.9626, -6.6922, -8.1125]]],\n",
      "       grad_fn=<LogSoftmaxBackward0>)\n",
      "torch.Size([2, 256, 1000])\n"
     ]
    }
   ],
   "source": [
    "transformer = Transformer(vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout)\n",
    "outputs = transformer(input_tokens, src_mask, tgt_mask, cross_mask)\n",
    "print(outputs)\n",
    "print(outputs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 259 tokens possíveis:\n",
    "# ASCII + <UNK> + <SOS> + <EOS>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
