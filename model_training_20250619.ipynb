{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74a08cdd",
   "metadata": {},
   "source": [
    "# Financial Explainer GPT Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7a07a7",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "78a140db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.init as init\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9d04b3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_length):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros((max_seq_length, d_model))\n",
    "        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)]\n",
    "    \n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0, \"Vetor de embedding precisa ser divisivel pelo número de cabeças da camada de atenção!\"\n",
    "        self.head_dim = d_model // num_heads\n",
    "        self.d_model, self.num_heads = d_model, num_heads\n",
    "        self.q = nn.Linear(d_model, d_model)\n",
    "        self.k = nn.Linear(d_model, d_model)\n",
    "        self.v = nn.Linear(d_model, d_model)\n",
    "        self.output_linear = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def split_heads(self, x, encoder_output=None):\n",
    "        # Entra Q, K, V com dimensão (batch_size, sequence_length, d_model)\n",
    "        # Reshape para (batch_size, sequence_length, num_heads, d_model)\n",
    "        # Reordering para (batch_size, num_heads, sequence_length, d_model)\n",
    "        if encoder_output is None:\n",
    "            x = torch.reshape(x, shape=(x.shape[0], x.shape[1], self.num_heads, self.head_dim)) #.contiguous()\n",
    "            x = x.permute(0, 2, 1, 3)\n",
    "        else:\n",
    "            raise NotImplementedError(\"Modelo ainda não compatível com Encoder.\")\n",
    "        return x\n",
    "\n",
    "    def compute_attention_scores(self, q_linear_out, k_linear_out, v_linear_out, mask=None):\n",
    "        qk_dot_product = torch.matmul(q_linear_out, k_linear_out.transpose(2, 3)) / self.head_dim ** 0.5\n",
    "\n",
    "        if mask is not None:\n",
    "            qk_dot_product = qk_dot_product.masked_fill(mask == 0, float('-inf'))\n",
    "\n",
    "        attn_scores = nn.functional.softmax(qk_dot_product, dim=-1)\n",
    "        attn_weighted_v = torch.matmul(attn_scores, v_linear_out)\n",
    "\n",
    "        return attn_weighted_v\n",
    "\n",
    "\n",
    "    def combine_heads(self, x):\n",
    "        x = x.permute(0, 2, 1, 3).contiguous()\n",
    "        return torch.reshape(x, shape=(x.shape[0], x.shape[1], int(x.shape[2] * x.shape[3])))\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        q_linear_out = self.split_heads(self.q(x))\n",
    "        k_linear_out = self.split_heads(self.k(x))\n",
    "        v_linear_out = self.split_heads(self.v(x))\n",
    "        \n",
    "        attn_weighted_v = self.compute_attention_scores(q_linear_out, k_linear_out, v_linear_out, mask=mask)\n",
    "        attn_weighted_v = self.combine_heads(attn_weighted_v)\n",
    "        return self.output_linear(attn_weighted_v)\n",
    "\n",
    "\n",
    "class FeedForwardSubLayer(nn.Module):\n",
    "    def __init__(self, d_model, hidden_size):\n",
    "        super().__init__()\n",
    "        self.ff_1 = nn.Linear(d_model, hidden_size)\n",
    "        self.ff_2 = nn.Linear(hidden_size, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.ff_2(self.relu(self.ff_1(x)))\n",
    "    \n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, d_model, hidden_size, num_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.feed_forward = FeedForwardSubLayer(d_model, hidden_size)\n",
    "        self.mha = MultiHeadAttention(d_model, num_heads) # nn.MultiheadAttention()\n",
    "        self.norm_1 = nn.LayerNorm(d_model)\n",
    "        self.norm_2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, tgt_mask):\n",
    "        x = self.norm_1(x + self.dropout(self.mha(x, mask=tgt_mask)))\n",
    "        x = self.norm_2(x + self.dropout(self.feed_forward(x)))\n",
    "        return x\n",
    "    \n",
    "\n",
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, max_sequence_length, n_layers, hidden_size, num_heads, dropout=0.1):\n",
    "        super(TransformerDecoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=d_model, padding_idx=0)\n",
    "        self.pe = PositionalEncoding(d_model, max_sequence_length)\n",
    "        self.layers = nn.ModuleList(\n",
    "            [DecoderBlock(d_model, hidden_size, num_heads, dropout) for _ in range(n_layers)]\n",
    "        )\n",
    "        self.output_layer = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x, tgt_mask):\n",
    "        x = self.embedding(x)\n",
    "        x = self.pe(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, tgt_mask)\n",
    "        out = self.output_layer(x)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0992120",
   "metadata": {},
   "source": [
    "## Dataset Prep"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8aba01",
   "metadata": {},
   "source": [
    "### Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579e062a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenizerChar:\n",
    "    def __init__(self):\n",
    "        self.chr_to_idx = {chr(v): v for v in range(1, 257)}\n",
    "        self.chr_to_idx['<SOS>'] = 257\n",
    "        self.chr_to_idx['<EOS>'] = 258\n",
    "        self.chr_to_idx['<PAD>'] = 0\n",
    "        self.chr_to_idx['<UNK>'] = 259\n",
    "        self.chr_to_idx['<EOP>'] = 260\n",
    "\n",
    "        self.idx_to_chr = {v: k for k, v in self.chr_to_idx.items()}\n",
    "\n",
    "        self.vocab_size = len(self.chr_to_idx.keys())\n",
    "\n",
    "    def encode(self, char):\n",
    "        if char in self.chr_to_idx.keys():\n",
    "            return self.chr_to_idx[char]\n",
    "        else:\n",
    "            return 259\n",
    "    \n",
    "    def decode(self, token_idx):\n",
    "        return self.idx_to_chr[token_idx]\n",
    "    \n",
    "    def sos_token(self):\n",
    "        return '<SOS>'\n",
    "    \n",
    "    def sos_token_idx(self):\n",
    "        return self.chr_to_idx['<SOS>']\n",
    "\n",
    "    def eos_token(self):\n",
    "        return '<EOS>'\n",
    "    \n",
    "    def eos_token_idx(self):\n",
    "        return self.chr_to_idx['<EOS>']\n",
    "    \n",
    "    def pad_token(self):\n",
    "        return '<PAD>'\n",
    "    \n",
    "    def pad_token_idx(self):\n",
    "        return self.chr_to_idx['<PAD>']\n",
    "    \n",
    "    def eop_token(self):\n",
    "        return '<EOP>'\n",
    "    \n",
    "    def eop_token_idx(self):\n",
    "        return self.chr_to_idx['<EOP>']\n",
    "    \n",
    "    def get_vocab_size(self):\n",
    "        return self.vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ef63b7",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db99ff71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class DatasetFinancial(Dataset):\n",
    "    def __init__(self):\n",
    "        pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
